{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import book and libraries"
      ],
      "metadata": {
        "id": "bhVj1mrq62Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/financial_sentiment_merged.csv')\n",
        "\n",
        "print(df[['Cleaned']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecKkzyuK66cw",
        "outputId": "06ac6308-0212-4eb4-9d66-90e626403a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Cleaned\n",
            "0  geosolutions technology leverage benefon gps s...\n",
            "1                           esi low real possibility\n",
            "2  last quarter 2010 componenta net sale doubled ...\n",
            "3  according finnishrussian chamber commerce majo...\n",
            "4  swedish buyout firm sold remaining percent sta...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize the cleaned sentences and build word frequencies"
      ],
      "metadata": {
        "id": "vjHF_BMg8HRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenize each sentence and build frequency count\n",
        "tokenized_sentences = df['Cleaned'].astype(str).apply(word_tokenize)\n",
        "word_freq = Counter()\n",
        "\n",
        "for tokens in tokenized_sentences:\n",
        "    word_freq.update(tokens)\n",
        "\n",
        "print(word_freq.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qr3F1hL8Kc7",
        "outputId": "d9740755-170e-4b7c-b182-9c05efecd075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eeenf', 3328), ('tsla', 3015), ('price', 1862), ('company', 1763), ('share', 1758), ('new', 1735), ('buy', 1709), ('today', 1670), ('day', 1427), ('time', 1307)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Create word2idx and idx2word dictionaries"
      ],
      "metadata": {
        "id": "_vzYtzNw9I7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = ['<PAD>', '<UNK>']\n",
        "all_words = special_tokens + [word for word, freq in word_freq.items() if freq >= 1]\n",
        "\n",
        "word2idx = {word: idx for idx, word in enumerate(all_words)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "print(f\"Vocabulary size (including special tokens): {len(word2idx)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQXtw-mq9KPZ",
        "outputId": "372e2528-a0d6-4318-f7e3-a03c81077f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (including special tokens): 36212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Vocabulary.json"
      ],
      "metadata": {
        "id": "NZ7h2c_LgApI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "vocab = {\n",
        "    \"word2idx\": word2idx,\n",
        "    \"idx2word\": idx2word\n",
        "}\n",
        "with open('vocabulary.json', 'w') as f:\n",
        "    json.dump(vocab, f)\n",
        "\n",
        "print(\"Vocabulary saved as 'vocabulary.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1sMWm50gFuS",
        "outputId": "4f7d8a71-14b3-4ce6-ace3-f19a802dba0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary saved as 'vocabulary.json'\n"
          ]
        }
      ]
    }
  ]
}