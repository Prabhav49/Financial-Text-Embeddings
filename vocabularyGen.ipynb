{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import book and libraries"
      ],
      "metadata": {
        "id": "bhVj1mrq62Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/financial_sentiment_merged.csv')\n",
        "\n",
        "print(df[['Cleaned']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecKkzyuK66cw",
        "outputId": "894b52b4-d11c-4890-fc8d-aae24c01bfa8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             Cleaned\n",
            "0  geosolutions technology leverage benefon gps s...\n",
            "1                           esi low real possibility\n",
            "2  last quarter 2010 componenta net sale doubled ...\n",
            "3  according finnishrussian chamber commerce majo...\n",
            "4  swedish buyout firm sold remaining percent sta...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize the cleaned sentences and build word frequencies"
      ],
      "metadata": {
        "id": "vjHF_BMg8HRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenize each sentence and build frequency count\n",
        "tokenized_sentences = df['Cleaned'].astype(str).apply(word_tokenize)\n",
        "word_freq = Counter()\n",
        "\n",
        "for tokens in tokenized_sentences:\n",
        "    word_freq.update(tokens)\n",
        "\n",
        "print(word_freq.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qr3F1hL8Kc7",
        "outputId": "d1d56c2a-cdf9-41a9-bb44-03eabcba0b61"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eeenf', 3328), ('tsla', 3015), ('price', 1862), ('company', 1763), ('share', 1758), ('new', 1735), ('buy', 1709), ('today', 1670), ('day', 1427), ('time', 1307)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Create word2idx and idx2word dictionaries"
      ],
      "metadata": {
        "id": "_vzYtzNw9I7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "special_tokens = ['<PAD>', '<UNK>']\n",
        "all_words = special_tokens + [word for word, freq in word_freq.items() if freq >= 1]\n",
        "\n",
        "word2idx = {word: idx for idx, word in enumerate(all_words)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "print(f\"Vocabulary size (including special tokens): {len(word2idx)}\")\n",
        "\n",
        "\n",
        "embeddings = np.random.rand(len(word2idx), 100)\n",
        "\n",
        "def find_nearest_neighbors(embeddings, target_word_index, num_neighbors=5):\n",
        "    \"\"\"\n",
        "    Finds the nearest neighbors of a given word based on cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        embeddings: A NumPy array containing word embeddings.\n",
        "        target_word_index: The index of the target word in the vocabulary.\n",
        "        num_neighbors: The number of nearest neighbors to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        A list of indices of the nearest neighbors.\n",
        "    \"\"\"\n",
        "    target_embedding = embeddings[target_word_index]\n",
        "    similarities = np.dot(embeddings, target_embedding) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(target_embedding))\n",
        "    nearest_indices = np.argsort(similarities)[-num_neighbors - 1:-1][::-1]  # Exclude the word itself\n",
        "    return nearest_indices\n",
        "\n",
        "nearest_neighbors = find_nearest_neighbors(embeddings, word2idx['bull'])\n",
        "print(f\"Nearest neighbors of 'stock': {[idx2word[idx] for idx in nearest_neighbors]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQXtw-mq9KPZ",
        "outputId": "f1e03e83-8f13-4336-afeb-a7c0bf6825d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (including special tokens): 36212\n",
            "Nearest neighbors of 'stock': ['gogol', 'slumping', 'modelsplaid', 'jacked', 'metric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving Vocabulary.json"
      ],
      "metadata": {
        "id": "NZ7h2c_LgApI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "vocab = {\n",
        "    \"word2idx\": word2idx,\n",
        "    \"idx2word\": idx2word\n",
        "}\n",
        "with open('vocabulary.json', 'w') as f:\n",
        "    json.dump(vocab, f)\n",
        "\n",
        "print(\"Vocabulary saved as 'vocabulary.json'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1sMWm50gFuS",
        "outputId": "5d762d1c-9173-4ab6-b574-a54a09341b34"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary saved as 'vocabulary.json'\n"
          ]
        }
      ]
    }
  ]
}